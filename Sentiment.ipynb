{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.listdir('./Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews_raw = pd.read_csv('./Dataset/amazon_reviews_us_Kitchen_v1_00.tsv', sep = '\\t',error_bad_lines=False)\n",
    "#reviews_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_raw = pd.read_csv('https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz', sep = '\\t',error_bad_lines=False)\n",
    "reviews_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4874890, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews_raw[['review_body','star_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0    3124759\n",
      "4.0     731733\n",
      "1.0     426900\n",
      "3.0     349547\n",
      "2.0     241948\n",
      "Name: star_rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(reviews['star_rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reviews:  4874890\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4874890 entries, 0 to 4874889\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count    Dtype  \n",
      "---  ------       --------------    -----  \n",
      " 0   review_body  4874644 non-null  object \n",
      " 1   star_rating  4874887 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 74.4+ MB\n",
      "review_body    246\n",
      "star_rating      3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#reviews[:5]\n",
    "print(\"Total Reviews: \",len(reviews))\n",
    "reviews.info(verbose = True,show_counts = True)\n",
    "print(reviews.isnull().sum())\n",
    "#create a new copy and remove null values from it\n",
    "reviews_cpy = reviews.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4874644, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_cpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1068298</th>\n",
       "      <td>use it all the time</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3115897</th>\n",
       "      <td>This simple wire cutter helps level any cake u...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4152788</th>\n",
       "      <td>This nutcracker does everything it advertises....</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body  star_rating\n",
       "1068298                                use it all the time          5.0\n",
       "3115897  This simple wire cutter helps level any cake u...          4.0\n",
       "4152788  This nutcracker does everything it advertises....          5.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 3 random rows\n",
    "reviews_cpy.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    3124595\n",
       "4.0     731701\n",
       "1.0     426870\n",
       "3.0     349539\n",
       "2.0     241939\n",
       "Name: star_rating, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_cpy['star_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling Reviews:\n",
    "## The reviews with rating 4,5 are labelled to be 1 and 1,2 are labelled as 0. Discard the reviews with rating 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         review_body  star_rating  label\n",
      "0                Beautiful.  Looks great on counter.          5.0      1\n",
      "1  I personally have 5 days sets and have also bo...          5.0      1\n",
      "2  Fabulous and worth every penny. Used for clean...          5.0      1\n",
      "3  A must if you love garlic on tomato marinara s...          5.0      1\n",
      "4  Worth every penny! Buy one now and be a pizza ...          5.0      1\n",
      "1    3856296\n",
      "0     668809\n",
      "Name: label, dtype: int64\n",
      "4525105\n"
     ]
    }
   ],
   "source": [
    "reviews_cpy.loc[:,'label'] = np.where(reviews_cpy['star_rating'] <= 2,0,1)\n",
    "reviews = reviews_cpy[reviews_cpy['star_rating'] != 3.0 ]\n",
    "print(reviews.head())\n",
    "print(reviews['label'].value_counts())\n",
    "print(reviews['label'].value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 3 or dropped  reviews count, Class 1 or positive reviews count, Class 0 or negative reviews count  : 349539, 3856296, 668809\n",
      "Class 3 or dropped  reviews count :  349539\n",
      "Class 0 or negative reviews count :  668809\n",
      "Class 1 or positive reviews count :  3856296\n"
     ]
    }
   ],
   "source": [
    "# 0 is negative sentiment classes \n",
    "# 1 is positive sentiment classes\n",
    "#Class 3 or dropped reviews count here\n",
    "print('Class 3 or dropped  reviews count, Class 1 or positive reviews count, Class 0 or negative reviews count  : {}, {}, {}'.format(len(reviews_cpy['label']) - len(reviews['star_rating']),reviews['label'].value_counts()[1],reviews['label'].value_counts()[0]))\n",
    "\n",
    "print('Class 3 or dropped  reviews count : ',len(reviews_cpy['label']) - len(reviews['star_rating']))\n",
    "print('Class 0 or negative reviews count : ',reviews['label'].value_counts()[0])\n",
    "print('Class 1 or positive reviews count : ',reviews['label'].value_counts()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We select 200000 reviews randomly with 100,000 positive and 100,000 negative reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_pos = reviews_cpy[reviews_cpy['label'] == 1].sample(100000,random_state = 101)\n",
    "reviews_neg = reviews_cpy[reviews_cpy['label'] == 0].sample(100000,random_state = 101)\n",
    "#print(reviews_pos.describe())\n",
    "#print(reviews_neg.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.concat([reviews_pos,reviews_neg],ignore_index = True)\n",
    "dataset = dataset.sample(frac = 1,random_state= 101).reset_index(drop = True)\n",
    "print(len(dataset))\n",
    "#dataset[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Convert the all reviews into the lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of reviews before data cleaning: 325.3601\n"
     ]
    }
   ],
   "source": [
    "#Char length of reviews before data cleaning\n",
    "avg_before_cleaning = dataset['review_body'].str.len().sum()/len(dataset['review_body'])\n",
    "print('Average character length of reviews before data cleaning:',avg_before_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    got these on a kindle fire special for six dol...\n",
      "1    only thing i can say about this product is gre...\n",
      "2    got this for my sister, and she loved it!!!  i...\n",
      "3     definitely keeps my son's lunch hot for 4 hours!\n",
      "4    ragalta countertop thermo electric hot & cold ...\n",
      "5    purchased two for a weekly men's prayer breakf...\n",
      "Name: review_body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataset['review_body'] = dataset['review_body'].str.lower()\n",
    "print(dataset.loc[:5,'review_body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the HTML and URLs from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25802"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all rows which have <> braces and could be html tags \n",
    "len([x for x in dataset['review_body'][dataset['review_body'].str.contains(\"<.*?>\")]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove html\n",
    "dataset['review_body'] = dataset['review_body'].apply(lambda x : BeautifulSoup(x,'html.parser').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if any tags remain\n",
    "len([x for x in dataset['review_body'][dataset['review_body'].str.contains(\"<.*?>\")]])\n",
    "#only remaining results are lone < or > markers spread around no html tags left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For URL removal\n",
    "def remove_url(text):\n",
    "    url = re.compile(r'(https?://\\S+|www\\.\\S+)|(\\S+\\.com\\S+)')\n",
    "    return url.sub(r'',text)\n",
    "dataset['review_body'] = dataset['review_body'].apply(remove_url)\n",
    "#check if any url still remain\n",
    "len([x for x in dataset['review_body'][dataset['review_body'].str.contains(\"http\\S+\")]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove every non alphabet chracter except apostrophe for contractions\n",
    "dataset['review_body'] = dataset['review_body'].replace(\"[^a-z ']\",'',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform contractions on the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be done before non alphabet characters are removed\n",
    "import contractions\n",
    "def contractionfunction(s):\n",
    "    return contractions.fix(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84331"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for contractions\n",
    "len([x for x in dataset['review_body'][dataset['review_body'].str.contains(\"[a-z]+'[a-z]+\")]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['review_body'] = dataset['review_body'].apply(contractionfunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After contractions are removed; delete all the apostrophe as non alphabet chars are to be removed\n",
    "dataset['review_body'] = dataset['review_body'].replace(\"[^a-z ]\",'',regex=True)\n",
    "#check if any more apostrophe words remain\n",
    "[print(x) for x in dataset['review_body'][dataset['review_body'].str.contains(\"[a-z]+'[a-z]+\")]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#search for any non alphabetical \n",
    "[print(x) for x in dataset['review_body'][dataset['review_body'].str.contains(\"[^a-z ]\")]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the extra spaces between the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['review_body_without_extra_spaces'] = dataset['review_body'].replace('\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if multiple spaces still exist\n",
    "[print(x) for x in dataset['review_body'][dataset['review_body_without_extra_spaces'].str.contains('\\s{2,}')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of reviews after data cleaning: 312.50965\n"
     ]
    }
   ],
   "source": [
    "#Char length of reviews after data cleaning\n",
    "avg_after_cleaning = dataset['review_body'].str.len().sum()/len(dataset['review_body'])\n",
    "print('Average character length of reviews after data cleaning:',avg_after_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length before and after data cleaning : 325.3601, 312.50965\n"
     ]
    }
   ],
   "source": [
    "print('Average character length before and after data cleaning : {}, {}'.format(avg_before_cleaning,avg_after_cleaning))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of reviews before data preprocessing: 312.50965\n"
     ]
    }
   ],
   "source": [
    "print('Average character length of reviews before data preprocessing:',avg_after_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "dataset['review_body_without_stopwords'] =  dataset['review_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['review_body_lemmatized'] =  dataset['review_body_without_stopwords'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>label</th>\n",
       "      <th>review_body_without_extra_spaces</th>\n",
       "      <th>review_body_without_stopwords</th>\n",
       "      <th>review_body_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28265</th>\n",
       "      <td>does not fit or work in a keurig model  when m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>does not fit or work in a keurig model when ma...</td>\n",
       "      <td>fit work keurig model making purchase neither ...</td>\n",
       "      <td>fit work keurig model making purchase neither ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62925</th>\n",
       "      <td>love them great quality</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>love them great quality</td>\n",
       "      <td>love great quality</td>\n",
       "      <td>love great quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48039</th>\n",
       "      <td>just what i expected size is a little big for ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>just what i expected size is a little big for ...</td>\n",
       "      <td>expected size little big works hoping get pain...</td>\n",
       "      <td>expected size little big work hoping get paint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125817</th>\n",
       "      <td>it is sharp but i do not think that its claim ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>it is sharp but i do not think that its claim ...</td>\n",
       "      <td>sharp think claim nothing stick true cheese st...</td>\n",
       "      <td>sharp think claim nothing stick true cheese st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review_body  star_rating  label  \\\n",
       "28265   does not fit or work in a keurig model  when m...          1.0      0   \n",
       "62925                             love them great quality          5.0      1   \n",
       "48039   just what i expected size is a little big for ...          5.0      1   \n",
       "125817  it is sharp but i do not think that its claim ...          2.0      0   \n",
       "\n",
       "                         review_body_without_extra_spaces  \\\n",
       "28265   does not fit or work in a keurig model when ma...   \n",
       "62925                             love them great quality   \n",
       "48039   just what i expected size is a little big for ...   \n",
       "125817  it is sharp but i do not think that its claim ...   \n",
       "\n",
       "                            review_body_without_stopwords  \\\n",
       "28265   fit work keurig model making purchase neither ...   \n",
       "62925                                  love great quality   \n",
       "48039   expected size little big works hoping get pain...   \n",
       "125817  sharp think claim nothing stick true cheese st...   \n",
       "\n",
       "                                   review_body_lemmatized  \n",
       "28265   fit work keurig model making purchase neither ...  \n",
       "62925                                  love great quality  \n",
       "48039   expected size little big work hoping get paint...  \n",
       "125817  sharp think claim nothing stick true cheese st...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of reviews after data preprocessing: 191.44271\n"
     ]
    }
   ],
   "source": [
    "#Char length of reviews after data preprocessing\n",
    "avg_after_preprocessing = dataset['review_body_lemmatized'].str.len().sum()/len(dataset['review_body_lemmatized'])\n",
    "print('Average character length of reviews after data preprocessing:',avg_after_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length before and after data preprocessing : 312.50965, 191.44271\n"
     ]
    }
   ],
   "source": [
    "print('Average character length before and after data preprocessing : {}, {}'.format(avg_after_cleaning,avg_after_preprocessing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into test and train set\n",
    "X_train,X_test,y_train,y_test = train_test_split(dataset['review_body_lemmatized'],dataset['label'],test_size = 0.2,random_state = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vetorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train vectorizer over train data and transform test data for later\n",
    "tfidf_train_data = tfidf_vetorizer.fit_transform(X_train)\n",
    "tfidf_test_data = tfidf_vetorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (160000, 106917)\n",
      "Test Data Shape: (40000, 106917)\n"
     ]
    }
   ],
   "source": [
    "print('Train Data Shape:',tfidf_train_data.shape)\n",
    "print('Test Data Shape:',tfidf_test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True values should be first argument and predicted values are second\n",
    "def print_metrics(y_actual,y_pred):\n",
    "    print(\"Accuracy Score: \",metrics.accuracy_score(y_actual,y_pred))\n",
    "    print(\"Precision Score: \",metrics.precision_score(y_actual, y_pred))\n",
    "    print(\"Recall Score: \",metrics.recall_score(y_actual, y_pred))\n",
    "    print(\"F1 Score :\",metrics.f1_score(y_actual, y_pred))\n",
    "    \n",
    "def print_metrics_comma(y_actual,y_pred):\n",
    "    print(\"Accuracy Score, Precision Score, Recall Score, F1 Score: {}, {}, {}, {} \".format(metrics.accuracy_score(y_actual,y_pred),metrics.precision_score(y_actual, y_pred),metrics.recall_score(y_actual, y_pred),metrics.f1_score(y_actual, y_pred)))\n",
    "\n",
    "def print_metrics_comma_det(y_actual_train,y_pred_train,y_actual_test,y_pred_test):\n",
    "    print(\"Accuracy Score, Precision Score, Recall Score, F1 Score Training Set followed by Test Set: {}, {}, {}, {}, {}, {}, {}, {} \".format(metrics.accuracy_score(y_actual_train,y_pred_train),metrics.precision_score(y_actual_train, y_pred_train),metrics.recall_score(y_actual_train, y_pred_train),metrics.f1_score(y_actual_train, y_pred_train),metrics.accuracy_score(y_actual_test,y_pred_test),metrics.precision_score(y_actual_test, y_pred_test),metrics.recall_score(y_actual_test, y_pred_test),metrics.f1_score(y_actual_test, y_pred_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_model(tfidf_train_data,y_train,tfidf_test_data,y_test):\n",
    "    perceptron_model = Perceptron()\n",
    "    perceptron_model.fit(tfidf_train_data,y_train)\n",
    "    print_metrics_comma_det(y_train,perceptron_model.predict(tfidf_train_data),y_test,perceptron_model.predict(tfidf_test_data))\n",
    "    print(\"Perceptron Metrics for Train Set\")\n",
    "    print_metrics(y_train,perceptron_model.predict(tfidf_train_data))\n",
    "    print(\"Perceptron Metrics for Test Set\")\n",
    "    print_metrics(y_test,perceptron_model.predict(tfidf_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Precision Score, Recall Score, F1 Score Training Set followed by Test Set: 0.9010875, 0.9249195992535635, 0.8731493396968978, 0.8982891810948728, 0.82455, 0.8513873052071456, 0.78553178698462, 0.8171348168221376 \n",
      "Perceptron Metrics for Train Set\n",
      "Accuracy Score:  0.9010875\n",
      "Precision Score:  0.9249195992535635\n",
      "Recall Score:  0.8731493396968978\n",
      "F1 Score : 0.8982891810948728\n",
      "Perceptron Metrics for Test Set\n",
      "Accuracy Score:  0.82455\n",
      "Precision Score:  0.8513873052071456\n",
      "Recall Score:  0.78553178698462\n",
      "F1 Score : 0.8171348168221376\n"
     ]
    }
   ],
   "source": [
    "perceptron_model(tfidf_train_data,y_train,tfidf_test_data,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160000, 106917)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearSVCModel(tfidf_train_data,y_train,tfidf_test_data,y_test):\n",
    "    svc_classifier = LinearSVC()\n",
    "    svc_classifier.fit(tfidf_train_data,y_train)\n",
    "    print_metrics_comma_det(y_train,svc_classifier.predict(tfidf_train_data),y_test,svc_classifier.predict(tfidf_test_data))\n",
    "    print(\"SVM Metrics for Train Set\")\n",
    "    print_metrics(y_train,svc_classifier.predict(tfidf_train_data))\n",
    "    print(\"SVM Metrics for Test Set\")\n",
    "    print_metrics(y_test,svc_classifier.predict(tfidf_test_data))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Precision Score, Recall Score, F1 Score Training Set followed by Test Set: 0.92510625, 0.9294729402261712, 0.9201014505428604, 0.9247634534849408, 0.872425, 0.8768387947651415, 0.8659886779219478, 0.8713799621928167 \n",
      "SVM Metrics for Train Set\n",
      "Accuracy Score:  0.92510625\n",
      "Precision Score:  0.9294729402261712\n",
      "Recall Score:  0.9201014505428604\n",
      "F1 Score : 0.9247634534849408\n",
      "SVM Metrics for Test Set\n",
      "Accuracy Score:  0.872425\n",
      "Precision Score:  0.8768387947651415\n",
      "Recall Score:  0.8659886779219478\n",
      "F1 Score : 0.8713799621928167\n"
     ]
    }
   ],
   "source": [
    "linearSVCModel(tfidf_train_data,y_train,tfidf_test_data,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logitClassifierModel(tfidf_train_data,y_train,tfidf_test_data,y_test):\n",
    "    logit_classifier = LogisticRegression(class_weight = 'balanced',max_iter = 2000)\n",
    "    logit_classifier.fit(tfidf_train_data,y_train)\n",
    "    print_metrics_comma_det(y_train,logit_classifier.predict(tfidf_train_data),y_test,logit_classifier.predict(tfidf_test_data))\n",
    "    print(\"Logistic Regression Metrics for Train Set\")\n",
    "    print_metrics(y_train,logit_classifier.predict(tfidf_train_data))\n",
    "    print(\"Logistic Regression Metrics for Test Set\")\n",
    "    print_metrics(y_test,logit_classifier.predict(tfidf_test_data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Precision Score, Recall Score, F1 Score Training Set followed by Test Set: 0.89536875, 0.9014230993632836, 0.8879421282124964, 0.8946318313706485, 0.87645, 0.8838623932934622, 0.8662391663744301, 0.8749620483756704 \n",
      "Logistic Regression Metrics for Train Set\n",
      "Accuracy Score:  0.89536875\n",
      "Precision Score:  0.9014230993632836\n",
      "Recall Score:  0.8879421282124964\n",
      "F1 Score : 0.8946318313706485\n",
      "Logistic Regression Metrics for Test Set\n",
      "Accuracy Score:  0.87645\n",
      "Precision Score:  0.8838623932934622\n",
      "Recall Score:  0.8662391663744301\n",
      "F1 Score : 0.8749620483756704\n"
     ]
    }
   ],
   "source": [
    "logitClassifierModel(tfidf_train_data,y_train,tfidf_test_data,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomialNBClassifierModel(tfidf_train_data,y_train,tfidf_test_data,y_test):\n",
    "    multinomialNBClassifier = MultinomialNB()\n",
    "    multinomialNBClassifier.fit(tfidf_train_data,y_train)\n",
    "    print_metrics_comma_det(y_train,multinomialNBClassifier.predict(tfidf_train_data),y_test,multinomialNBClassifier.predict(tfidf_test_data))\n",
    "    print(\"Multinomial Naieve Bayes Metrics for Train Set\")\n",
    "    print_metrics(y_train,multinomialNBClassifier.predict(tfidf_train_data))\n",
    "    print(\"Multinomial Naieve Bayes Metrics for Test Set\")\n",
    "    print_metrics(y_test,multinomialNBClassifier.predict(tfidf_test_data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Precision Score, Recall Score, F1 Score Training Set followed by Test Set: 0.87338125, 0.8851704852967707, 0.8582191181798873, 0.8714864787713855, 0.85085, 0.8677282043197225, 0.8272130654776815, 0.8469864067709669 \n",
      "Multinomial Naieve Bayes Metrics for Train Set\n",
      "Accuracy Score:  0.87338125\n",
      "Precision Score:  0.8851704852967707\n",
      "Recall Score:  0.8582191181798873\n",
      "F1 Score : 0.8714864787713855\n",
      "Multinomial Naieve Bayes Metrics for Test Set\n",
      "Accuracy Score:  0.85085\n",
      "Precision Score:  0.8677282043197225\n",
      "Recall Score:  0.8272130654776815\n",
      "F1 Score : 0.8469864067709669\n"
     ]
    }
   ],
   "source": [
    "multinomialNBClassifierModel(tfidf_train_data,y_train,tfidf_test_data,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Tests\n",
    "1. Include Review Title into the review body and then preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_incl_heading = reviews_raw[['review_body','star_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_incl_heading['review_body'] = reviews_raw['review_headline'] + \" \" + reviews_raw['review_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beautiful. Looks great on counter Beautiful.  ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Awesome &amp; Self-ness I personally have 5 days s...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fabulous and worth every penny Fabulous and wo...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Five Stars A must if you love garlic on tomato...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Better than sex Worth every penny! Buy one now...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  star_rating\n",
       "0  Beautiful. Looks great on counter Beautiful.  ...          5.0\n",
       "1  Awesome & Self-ness I personally have 5 days s...          5.0\n",
       "2  Fabulous and worth every penny Fabulous and wo...          5.0\n",
       "3  Five Stars A must if you love garlic on tomato...          5.0\n",
       "4  Better than sex Worth every penny! Buy one now...          5.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_incl_heading.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(dataset):\n",
    "    dataset = dataset.dropna()\n",
    "    dataset.loc[:,'label'] = np.where(dataset['star_rating'] <= 2,0,1)\n",
    "    dataset = dataset[dataset['star_rating'] != 3.0 ]\n",
    "    reviews_pos = dataset[dataset['label'] == 1].sample(100000,random_state = 101)\n",
    "    reviews_neg = dataset[dataset['label'] == 0].sample(100000,random_state = 101)\n",
    "    dataset = pd.concat([reviews_pos,reviews_neg],ignore_index = True)\n",
    "    dataset = dataset.sample(frac = 1,random_state= 101).reset_index(drop = True)\n",
    "    print(len(dataset))\n",
    "    print(dataset.head())\n",
    "    dataset['review_body'] = dataset['review_body'].str.lower()\n",
    "    dataset['review_body'] = dataset['review_body'].apply(remove_url)\n",
    "    dataset['review_body'] = dataset['review_body'].apply(lambda x : BeautifulSoup(x,'html.parser').get_text())\n",
    "    dataset['review_body'] = dataset['review_body'].replace(\"[^a-z ']\",'',regex=True)\n",
    "    dataset['review_body'] = dataset['review_body'].apply(contractionfunction)\n",
    "    dataset['review_body'] = dataset['review_body'].replace(\"[^a-z ]\",'',regex=True)\n",
    "    dataset['review_body'] = dataset['review_body'].replace('\\s+', ' ', regex=True)\n",
    "    print('Average character length of reviews after data cleaning:',dataset['review_body'].str.len().sum()/len(dataset['review_body']))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "                                         review_body  star_rating  label\n",
      "0  Beauty and Durability rolled into one. I thoug...          5.0      1\n",
      "1  Great napkin holder. Very pretty napkin holder...          5.0      1\n",
      "2  Love It It was exactly what I was looking for....          5.0      1\n",
      "3  Perfect Size & Shape I've waxed rhapsodic abou...          5.0      1\n",
      "4  Great Tool My husband decided to start juicing...          5.0      1\n",
      "Average character length of reviews after data cleaning: 330.664615\n"
     ]
    }
   ],
   "source": [
    "reviews_incl_heading = clean(reviews_incl_heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    dataset['review_body_without_stopwords'] =  dataset['review_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    dataset['review_body_lemmatized'] =  dataset['review_body_without_stopwords'].apply(lemmatize_text)\n",
    "    print('Average character length of reviews after data preprocessing:',dataset['review_body_lemmatized'].str.len().sum()/len(dataset['review_body_lemmatized']))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of reviews after data preprocessing: 207.432255\n"
     ]
    }
   ],
   "source": [
    "reviews_incl_heading = preprocess(reviews_incl_heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(reviews_incl_heading['review_body_lemmatized'],reviews_incl_heading['label'],test_size = 0.2,random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(X_train,X_test):\n",
    "    tfidf_vetorizer = TfidfVectorizer()\n",
    "    tfidf_train_data = tfidf_vetorizer.fit_transform(X_train)\n",
    "    tfidf_test_data = tfidf_vetorizer.transform(X_test)\n",
    "    return tfidf_train_data,tfidf_test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_train,rev_test = vectorize(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Precision Score, Recall Score, F1 Score Training Set followed by Test Set: 0.9461375, 0.9439009534227504, 0.9487125026549557, 0.9463006118913798, 0.8949, 0.8940235058764692, 0.895546315314864, 0.8947842626889578 \n",
      "Perceptron Metrics for Train Set\n",
      "Accuracy Score:  0.9461375\n",
      "Precision Score:  0.9439009534227504\n",
      "Recall Score:  0.9487125026549557\n",
      "F1 Score : 0.9463006118913798\n",
      "Perceptron Metrics for Test Set\n",
      "Accuracy Score:  0.8949\n",
      "Precision Score:  0.8940235058764692\n",
      "Recall Score:  0.895546315314864\n",
      "F1 Score : 0.8947842626889578\n"
     ]
    }
   ],
   "source": [
    "perceptron_model(rev_train,y_train,rev_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Precision Score, Recall Score, F1 Score Training Set followed by Test Set: 0.9603375, 0.9610942173167649, 0.9595572158572696, 0.9603251015942483, 0.925675, 0.9264912633058847, 0.9244025850408296, 0.9254457456679288 \n",
      "SVM Metrics for Train Set\n",
      "Accuracy Score:  0.9603375\n",
      "Precision Score:  0.9610942173167649\n",
      "Recall Score:  0.9595572158572696\n",
      "F1 Score : 0.9603251015942483\n",
      "SVM Metrics for Test Set\n",
      "Accuracy Score:  0.925675\n",
      "Precision Score:  0.9264912633058847\n",
      "Recall Score:  0.9244025850408296\n",
      "F1 Score : 0.9254457456679288\n"
     ]
    }
   ],
   "source": [
    "linearSVCModel(rev_train,y_train,rev_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Precision Score, Recall Score, F1 Score Training Set followed by Test Set: 0.93736875, 0.9405088456602159, 0.93386973850248, 0.937177534119476, 0.92865, 0.9316679283371183, 0.9248534642552978, 0.9282481898632341 \n",
      "Logistic Regression Metrics for Train Set\n",
      "Accuracy Score:  0.93736875\n",
      "Precision Score:  0.9405088456602159\n",
      "Recall Score:  0.93386973850248\n",
      "F1 Score : 0.937177534119476\n",
      "Logistic Regression Metrics for Test Set\n",
      "Accuracy Score:  0.92865\n",
      "Precision Score:  0.9316679283371183\n",
      "Recall Score:  0.9248534642552978\n",
      "F1 Score : 0.9282481898632341\n"
     ]
    }
   ],
   "source": [
    "logitClassifierModel(rev_train,y_train,rev_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Precision Score, Recall Score, F1 Score Training Set followed by Test Set: 0.9081375, 0.9189653624693828, 0.8953135346518573, 0.9069852801579568, 0.89335, 0.9051158948944298, 0.8783628074745754, 0.8915386962269907 \n",
      "Multinomial Naieve Bayes Metrics for Train Set\n",
      "Accuracy Score:  0.9081375\n",
      "Precision Score:  0.9189653624693828\n",
      "Recall Score:  0.8953135346518573\n",
      "F1 Score : 0.9069852801579568\n",
      "Multinomial Naieve Bayes Metrics for Test Set\n",
      "Accuracy Score:  0.89335\n",
      "Precision Score:  0.9051158948944298\n",
      "Recall Score:  0.8783628074745754\n",
      "F1 Score : 0.8915386962269907\n"
     ]
    }
   ],
   "source": [
    "multinomialNBClassifierModel(rev_train,y_train,rev_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Tests \n",
    "2. Remove features/terms that only occur 1 time or at most 2 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_reduced_terms = reviews_raw[['review_body','star_rating']]\n",
    "reviews_reduced_terms['review_body'] = reviews_raw['review_headline'] + \" \" + reviews_raw['review_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "                                         review_body  star_rating  label\n",
      "0  Beauty and Durability rolled into one. I thoug...          5.0      1\n",
      "1  Great napkin holder. Very pretty napkin holder...          5.0      1\n",
      "2  Love It It was exactly what I was looking for....          5.0      1\n",
      "3  Perfect Size & Shape I've waxed rhapsodic abou...          5.0      1\n",
      "4  Great Tool My husband decided to start juicing...          5.0      1\n",
      "Average character length of reviews after data cleaning: 330.664615\n"
     ]
    }
   ],
   "source": [
    "reviews_reduced_terms = clean(reviews_reduced_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize2(X_train,X_test):\n",
    "    tfidf_vetorizer = TfidfVectorizer(min_df = 2)\n",
    "    tfidf_train_data = tfidf_vetorizer.fit_transform(X_train)\n",
    "    tfidf_test_data = tfidf_vetorizer.transform(X_test)\n",
    "    return tfidf_train_data,tfidf_test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of reviews after data preprocessing: 207.432255\n"
     ]
    }
   ],
   "source": [
    "reviews_reduced_terms = preprocess(reviews_reduced_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(reviews_reduced_terms['review_body_lemmatized'],reviews_reduced_terms['label'],test_size = 0.2,random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_train,red_test = vectorize2(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160000, 35746)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Precision Score, Recall Score, F1 Score Training Set followed by Test Set: 0.9348125, 0.9258525125720368, 0.945401616711853, 0.9355249493101231, 0.897875, 0.8906111603188662, 0.9067181002955764, 0.8985924583571234 \n",
      "Perceptron Metrics for Train Set\n",
      "Accuracy Score:  0.9348125\n",
      "Precision Score:  0.9258525125720368\n",
      "Recall Score:  0.945401616711853\n",
      "F1 Score : 0.9355249493101231\n",
      "Perceptron Metrics for Test Set\n",
      "Accuracy Score:  0.897875\n",
      "Precision Score:  0.8906111603188662\n",
      "Recall Score:  0.9067181002955764\n",
      "F1 Score : 0.8985924583571234\n"
     ]
    }
   ],
   "source": [
    "perceptron_model(red_train,y_train,red_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Precision Score, Recall Score, F1 Score Training Set followed by Test Set: 0.95314375, 0.9540231323853394, 0.9522232911455666, 0.953122362077698, 0.925625, 0.9266981511254019, 0.9240519012073544, 0.9253731343283582 \n",
      "SVM Metrics for Train Set\n",
      "Accuracy Score:  0.95314375\n",
      "Precision Score:  0.9540231323853394\n",
      "Recall Score:  0.9522232911455666\n",
      "F1 Score : 0.953122362077698\n",
      "SVM Metrics for Test Set\n",
      "Accuracy Score:  0.925625\n",
      "Precision Score:  0.9266981511254019\n",
      "Recall Score:  0.9240519012073544\n",
      "F1 Score : 0.9253731343283582\n"
     ]
    }
   ],
   "source": [
    "linearSVCModel(red_train,y_train,red_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Precision Score, Recall Score, F1 Score Training Set followed by Test Set: 0.93619375, 0.9390498465871938, 0.9330076587663514, 0.9360190018989366, 0.9289, 0.9324440402203021, 0.9245027804218225, 0.928456429865164 \n",
      "Logistic Regression Metrics for Train Set\n",
      "Accuracy Score:  0.93619375\n",
      "Precision Score:  0.9390498465871938\n",
      "Recall Score:  0.9330076587663514\n",
      "F1 Score : 0.9360190018989366\n",
      "Logistic Regression Metrics for Test Set\n",
      "Accuracy Score:  0.9289\n",
      "Precision Score:  0.9324440402203021\n",
      "Recall Score:  0.9245027804218225\n",
      "F1 Score : 0.928456429865164\n"
     ]
    }
   ],
   "source": [
    "logitClassifierModel(red_train,y_train,red_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Precision Score, Recall Score, F1 Score Training Set followed by Test Set: 0.90353125, 0.9069389503388848, 0.8994490186034308, 0.903178456500875, 0.894475, 0.897394465764492, 0.8903361555032313, 0.8938513768389287 \n",
      "Multinomial Naieve Bayes Metrics for Train Set\n",
      "Accuracy Score:  0.90353125\n",
      "Precision Score:  0.9069389503388848\n",
      "Recall Score:  0.8994490186034308\n",
      "F1 Score : 0.903178456500875\n",
      "Multinomial Naieve Bayes Metrics for Test Set\n",
      "Accuracy Score:  0.894475\n",
      "Precision Score:  0.897394465764492\n",
      "Recall Score:  0.8903361555032313\n",
      "F1 Score : 0.8938513768389287\n"
     ]
    }
   ],
   "source": [
    "multinomialNBClassifierModel(red_train,y_train,red_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
